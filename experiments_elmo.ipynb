{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ddc924010d37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Spacy \n",
    "import spacy\n",
    "#from spacy.lang.en import English\n",
    "#from spacy import displacy\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#import Utils\n",
    "from utils import get_corpus_dataframe\n",
    "\n",
    "from IPython.display import HTML\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retireve sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences(save_as_filename):\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_md')\n",
    "    nlp.max_length = 1520000\n",
    "    MAX_WORDS_IN_SENTENCE = 80\n",
    "    MIN_WORDS_IN_SENTENCE = 3\n",
    "    punctuation = '!\"#$%&()*+:;<=>?@[\\\\]^_`{|}~●'\n",
    "    # Load data \n",
    "    # Import Dataset into a Pandas Dataframe\n",
    "    df = get_corpus_dataframe(eu_only=False)\n",
    "\n",
    "    ## Clean Text\n",
    "    # remove punctuation \n",
    "    df['clean_content'] = df['clean_content'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
    "\n",
    "    # convert text to lowercase\n",
    "    df['clean_content'] = df['clean_content'].str.lower()\n",
    "\n",
    "    ## Load Sentences\n",
    "    sentences = [sent.string.strip() for text in df.clean_content.tolist() for sent in nlp(text).sents if len(sent) in range(MIN_WORDS_IN_SENTENCE,MAX_WORDS_IN_SENTENCE)]\n",
    "\n",
    "    #save \n",
    "    if (save_as_filename):\n",
    "        with open(save_as_filename, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(sentences, fp)\n",
    "    \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_sentences_filename ='./preprocessed_text/elmo_sentences.sav'\n",
    "if os.path.isfile(elmo_sentences_filename):\n",
    "    with open(elmo_sentences_filename, \"rb\") as fp:   # Unpickling\n",
    "        sentences = pickle.load(fp)\n",
    "else:\n",
    "    # Create sentences\n",
    "    sentences = create_sentences(save_as_filename=elmo_sentences_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9201\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # calculate embeddings via tfhub\n",
    "url = \"https://tfhub.dev/google/elmo/3\"\n",
    "elmo = hub.load(url)\n",
    "\n",
    "def elmo_vectors(x):\n",
    "    tensor_list = tf.convert_to_tensor(x)\n",
    "    embeddings = elmo.signatures['default'](tensor_list)[\"default\"]\n",
    "    return embeddings\n",
    "        \n",
    "\n",
    "# function to return the elmo embeddings\n",
    "def get_sentences_vectors():\n",
    "    \n",
    "    elmo_embeddings_filename ='./saved_state/elmo_embeddings.pkl'\n",
    "    # check if the embeddings are already available in pickle file\n",
    "    if os.path.isfile(elmo_embeddings_filename):\n",
    "        with open(elmo_embeddings_filename, \"rb\") as fp:   # Unpickling\n",
    "            sentences_embeddings = pickle.load(fp)\n",
    "    else:\n",
    "       \n",
    "        # split in batches of 500\n",
    "        list_batch_sentences = [sentences[i:i+500] for i in range(0,len(sentences),500)]\n",
    "\n",
    "        # Extract ELMo embeddings\n",
    "        elmo_embeddings = [elmo_vectors(x) for x in list_batch_sentences]\n",
    "\n",
    "        # concentrate ELMo embeddings\n",
    "        #flatten the lists\n",
    "        sentences_embeddings = [y for x in elmo_embeddings for y in x]\n",
    "\n",
    "        # pickle\n",
    "        with open(elmo_embeddings_filename, \"wb\") as fp:   #Pickling\n",
    "            pickle.dump(sentences_embeddings, fp)\n",
    "    \n",
    "    return sentences_embeddings\n",
    "\n",
    "sentences_vectors = get_sentences_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#search_term='code of ethics' # param\n",
    "search_term='dangers from artificial intelignce for democracy'\n",
    "embeddings_search_vectors = elmo.signatures['default'](tf.convert_to_tensor([search_term]))['default']\n",
    "\n",
    "cosine_similarities = pd.Series(cosine_similarity(embeddings_search_vectors,sentences_vectors).flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Results:</h3><p style=\"font-family:verdana; font-size:110%;\">  experts list the real <b>dangers</b> of <b>artificial</b> intelligence, dave gershgorn feb. foreseen developments of <b>ia</b> supports human beings <b>in</b> some specific areas such as speech recognition.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  ethical principles <b>for</b> <b>artificial</b> intelligence work on <b>artificial</b> intelligence raises <b>a</b> number of ethical issues.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  however the large-scale application of <b>artificial</b> intelligence also comes with uncertainties and threats related to wellbeing, such as the fear of the loss of jobs.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  ethical principles <b>for</b> <b>artificial</b> intelligence .</p><hr><p style=\"font-family:verdana; font-size:110%;\">  ethical principles <b>for</b> <b>artificial</b> intelligence .</p><hr><p style=\"font-family:verdana; font-size:110%;\">  the potential of ai <b>in</b> public administration <b>artificial</b> intelligence today the potential of <b>artificial</b> intelligence <b>for</b> public administration is manifold.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  — <b>for</b> inclusive and diverse <b>artificial</b> intelligence <b>artificial</b> intelligence cannot become another driving force <b>for</b> exclusion</p><hr><p style=\"font-family:verdana; font-size:110%;\">  challenges <b>for</b> the use of <b>artificial</b> intelligence <b>in</b> denmark • • • need <b>for</b> common guidelines and <b>an</b> ethical framework <b>for</b> <b>artificial</b> intelligence <b>artificial</b> intelligence entails <b>a</b> new way of making decisions.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  others are new issues regarding the responsibility <b>for</b> decision-making, transparency and discrimination, <b>for</b> example.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  the role of <b>artificial</b> intelligence <b>in</b> finlands wellbeing these days, it seems that <b>artificial</b> intelligence is everywhere.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  furthermore, <b>artificial</b> intelligence raises <b>a</b> number of ethical issues relating to the relationship between, on the one hand the advantages <b>from</b> using new technologies, and, on other hand, consideration of people’s basic rights, due process, and fundamental social values.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  the goals of the government are that • ethical principles are incorporated <b>in</b> the development and use of <b>artificial</b> intelligence to secure respect <b>for</b> individuals and their rights, and <b>for</b> democracy.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  ’s approach to <b>artificial</b> intelligence is <b>a</b> combination of the us and chinese models.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  thirdly, the <b>dangers</b> of <b>a</b> blind implementation of</p><hr><p style=\"font-family:verdana; font-size:110%;\">  transparent use of <b>artificial</b> intelligence</p><hr><p style=\"font-family:verdana; font-size:110%;\">  — <b>artificial</b> intelligence working <b>for</b> <b>a</b> more ecological economy <b>an</b> its <b>from</b> carving out <b>a</b> meaningful role <b>for</b> intelligence also means <b>artificial</b> sustainability, addressing especially ecological standpoint.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  finland will make use of <b>artificial</b> intelligence boldly <b>in</b> all areas of society – <b>from</b> health care to the manufacturing industry – ethically and openly.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  however, <b>in</b> some areas, denmark has particularly good preconditions <b>for</b> using <b>artificial</b> intelligence, <b>for</b> example good data.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  the future of <b>artificial</b> intelligence surely depends on its exposure to these different technological developments.</p><hr><p style=\"font-family:verdana; font-size:110%;\">  <b>artificial</b> intelligence technologies could also play <b>a</b> decisive role <b>in</b> the battle against functional illiteracy.</p><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_returned = \"20\" #@param [1, 2, 3]\n",
    "\n",
    "output =\"\"\n",
    "for i,j in cosine_similarities.nlargest(int(results_returned)).iteritems():\n",
    "  output +='<p style=\"font-family:verdana; font-size:110%;\"> '\n",
    "  for i in sentences[i].split():\n",
    "    if i.lower() in search_term:\n",
    "      output += \" <b>\"+str(i)+\"</b>\"\n",
    "    else:\n",
    "      output += \" \"+str(i)\n",
    "  output += \"</p><hr>\"\n",
    "    \n",
    "output = '<h3>Results:</h3>'+output\n",
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in batches of 500\n",
    "sentences_chunks = [sentences[i:i+500] for i in range(0,len(sentences),500)]\n",
    "\n",
    "# Extract ELMo embeddings\n",
    "sentences_upper = [y.upper() for x in sentences_chunks for y in x]\n",
    "\n",
    "# # concentrate ELMo embeddings\n",
    "# #flatten the lists\n",
    "# sentences_new = [y for x in sentences_upper for y in x]\n",
    "\n",
    "for x in [1,2,3,4,5]: #stupid way for fun\n",
    "    i = np.random.randint(0,len(sentences))\n",
    "    print(sentences[i])\n",
    "    print(sentences_upper[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
